\chapter[Experimentos y resultados]{Experimentos y resultados}
\label{ch:exp_result}

\section{Base de datos MMI}
\label{exp:bdd}
Para poder realizar los experimentos utilizando algoritmo propuesto en el Capítulo~\ref{ch:algoritmo}, se utilizo una base de datos preparada para el reconocimiento facial y de expresiones. Creada por Pantic \etal~\cite{Pantic2005}, MMI es una base de datos multiuso que en esta instancia se utilizo para el reconocimiento de expresiones faciales. Contiene mas de 1000 ejemplos clasificados tanto en imágenes como vídeos; 19 sujetos de prueba; Las edades de los sujetos de prueba varían entre los 19 y los 62 años;  contiene tanto hombres como mujeres, además de tres razas étnicas distintas; Y por último tanto las imágenes como los vídeos están grabados de forma frontal y lateral con respecto al rostro del sujeto. En esta ocasión utilizamos los vídeos grabados de forma frontal, y siendo cada uno de estos etiquetado con una clase distinta para cada expresión facial: Ira~(E1), Asco~(E2), Miedo~(E3), Alegría~(E4), Tristeza~(E5) y Sorpresa~(E6).

\section{Experimentos}
\label{exp:exp}

Para poder probar la efectividad y buen modelado del algoritmo propuesto en el Capítulo~\ref{ch:algoritmo}, se preparo una pila de experimentos, los cuales permitieron realizar una revisión del modelado y precisión de método.
Se prepararon pruebas para cada uno de los pasos del algoritmo, estas pruebas nos permitieron elegir los mejores valores para cada una de las variables a utilizar. Cabe destacar que todos los experimentos realizados sobre el algoritmo, se realizaron en dos instancias de la base de datos MMI, la primera instancia consta de una versión en escala de grises de cada uno de los vídeos, la segunda es una versión a la cual se aplico la técnica LBP descrita en la Sección~\ref{sec:lbp}.

Para la etapa de Extracción de micro-descriptores propuesta en la Sección~\ref{algoritmo:ext_rayos}, se realizaron pruebas que permitieron ver el modelado de los \textit{rayos de flujo} con respecto al movimiento de los pixeles a lo largo de los vídeos, y la elección del tamaño de la Región de soporte $R$ y la Ventana de búsqueda $W$.

En la etapa de Normalización de micro-descriptores, propuesto en la Sección~\ref{algoritmo:normalizacion}, se realizaron pruebas con distintos tamaños de $N$ (Variable utilizada para describir el nuevo tamaño de los \textit{rayos de flujo}) y se realizaron comparaciones con respecto a la Asertividad (\textit{Accuracy} en inglés) de cada valor.

Para la creación de macro-descriptores, propuesta en la Sección~\ref{sec:macro-descriptores}, se prepararon dos experimentos distintos, primero se realizó un estudio del agrupamiento de los \textit{rayos} en el rostro para cada uno de los vídeos con distintos valores $K$, donde $K$ indica la cantidad de grupos de \textit{rayos de flujo} existentes y de que manera se distribuyen en el rostro. El segundo experimento que se realizó fue calcular la Asertividad (\textit{Accuracy} en inglés) para distintos valores de la variable $K$.

Por último en la etapa de entrenamiento y posterior clasificación, explicada en la Sección~\ref{sec:clasificacion}, se realizaron pruebas con los distintos \textit{Kernel} y sus respectivas variables, los cuales son recibidos por SVM para la generación del modelo.


\definecolor{lightgray}{gray}{0.9}
\newlength{\sz}
\setlength{\sz}{2cm}
\begin{table}[t!]
	\centering
	\begin{tabular}{ >{\centering\arraybackslash}m{.2cm}  >{\centering\arraybackslash}m{1.9cm}  >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.5cm}  >{\centering\arraybackslash}m{1.9cm}  >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.5cm}  }
		\hline\noalign{\smallskip}
		& \multicolumn{3}{ c }{Imágenes sin codificación} & \multicolumn{3}{ c }{Imágenes con LBP}\\
		\hline\noalign{\smallskip}
		\raisebox{.8cm}{\rotatebox{90}{\centering\parbox{2cm}{E1--Ira}}} & \includegraphics[height=\sz]{Figuras/resultados/E1/E1.png} & \includegraphics[height=\sz]{Figuras/resultados/E1/E1_YT.png} & \includegraphics[height=\sz]{Figuras/resultados/E1/E1_XT.png} & \includegraphics[height=\sz]{Figuras/resultados/E1/E1_LBP.png} & \includegraphics[height=\sz]{Figuras/resultados/E1/E1_LBP_YT.png} & \includegraphics[height=\sz]{Figuras/resultados/E1/E1_LBP_XT.png} \\
		
		\raisebox{.4cm}{\rotatebox{90}{\centering\parbox{2cm}{E2--Asco}}}& \includegraphics[height=\sz]{Figuras/resultados/E2/E2.png} & \includegraphics[height=\sz]{Figuras/resultados/E2/E2_YT.png} & \includegraphics[height=\sz]{Figuras/resultados/E2/E2_XT.png} & \includegraphics[height=\sz]{Figuras/resultados/E2/E2_LBP.png} & \includegraphics[height=\sz]{Figuras/resultados/E2/E2_LBP_YT.png} & \includegraphics[height=\sz]{Figuras/resultados/E2/E2_LBP_XT.png} \\
		
		\raisebox{.3cm}{\rotatebox{90}{\centering\parbox{2cm}{E3--Miedo}}}& \includegraphics[height=\sz]{Figuras/resultados/E3/E3.png} & \includegraphics[height=\sz]{Figuras/resultados/E3/E3_YT.png} & \includegraphics[height=\sz]{Figuras/resultados/E3/E3_XT.png} & \includegraphics[height=\sz]{Figuras/resultados/E3/E3_LBP.png} & \includegraphics[height=\sz]{Figuras/resultados/E3/E3_LBP_YT.png} & \includegraphics[height=\sz]{Figuras/resultados/E3/E3_LBP_XT.png} \\
		
		\raisebox{.2cm}{\rotatebox{90}{\centering\parbox{2cm}{E4--Alegría}}}& \includegraphics[height=\sz]{Figuras/resultados/E4/E4.png} & \includegraphics[height=\sz]{Figuras/resultados/E4/E4_YT.png} & \includegraphics[height=\sz]{Figuras/resultados/E4/E4_XT.png} & \includegraphics[height=\sz]{Figuras/resultados/E4/E4_LBP.png} & \includegraphics[height=\sz]{Figuras/resultados/E4/E4_LBP_YT.png} & \includegraphics[height=\sz]{Figuras/resultados/E4/E4_LBP_XT.png} \\
		
		\raisebox{0cm}{\rotatebox{90}{\centering\parbox{2cm}{E5--Tristeza}}}& \includegraphics[height=\sz]{Figuras/resultados/E5/E5.png} & \includegraphics[height=\sz]{Figuras/resultados/E5/E5_YT.png} & \includegraphics[height=\sz]{Figuras/resultados/E5/E5_XT.png} & \includegraphics[height=\sz]{Figuras/resultados/E5/E5_LBP.png} & \includegraphics[height=\sz]{Figuras/resultados/E5/E5_LBP_YT.png} & \includegraphics[height=\sz]{Figuras/resultados/E5/E5_LBP_XT.png} \\
		
		\raisebox{0cm}{\rotatebox{90}{\centering\parbox{2.1cm}{E6--Sorpresa}}}& \includegraphics[height=\sz]{Figuras/resultados/E6/E6.png} & \includegraphics[height=\sz]{Figuras/resultados/E6/E6_YT.png} & \includegraphics[height=\sz]{Figuras/resultados/E6/E6_XT.png} & \includegraphics[height=\sz]{Figuras/resultados/E6/E6_LBP.png} & \includegraphics[height=\sz]{Figuras/resultados/E6/E6_LBP_YT.png} & \includegraphics[height=\sz]{Figuras/resultados/E6/E6_LBP_XT.png} \\
		& (a) & (b) & (c) & (d) & (e) & (f) \\

		
	\end{tabular}
	\caption{Tabla comparativa de la extracción de micro-decriptores con vídeos codificados con LBP y sin codificar. Cada fila representa una expresión facial distinta. Las columnas (a) y (d) son el primer cuadro del vídeo, (b) y (e) representan el plano XT, y (c) y (f) representan el plano YT. }
	\label{tabla:comparacion_rayos}
\end{table}


\subsection{Extracción de micro-descriptores}
\label{exp:micro-descriptores}

La extracción de micro-descriptores, etapa propuesta en al Sección~\ref{algoritmo:ext_rayos}, consiste en modelar el movimiento de cada uno de los píxeles a lo largo de todo el vídeo, este modelado es llamado \textit{rayo de flujo}. Para poder extraer un \textit{rayo} se utilizan dos ventanas: la región de soporte y la ventana de búsqueda. Estas regiones permiten calcular a que píxel $(x',y')$ en un tiempo $t+1$ se desplazo el píxel $(x,y)$ en un tiempo $t$. Luego de calcular el movimiento de un píxel de $t$ a $t+1$ se crea un \textit{rayo de soporte}, que es la composición mínima para la creación de los \textit{rayos de flujo}. Para ver en detalle este proceso revisar la Sección~\ref{algoritmo:ext_rayos}.

Para poder evaluar el real modelado de los \textit{rayos} sobre el movimiento de los pixeles, se realizó una extracción de los planos $XT$ y $YT$ del vídeo. El plano $XT$ es una imagen extraída del vídeo en un pixel $(x,y)$, donde cada columna representa un cuadro distinto, cada una de estas son extraídas de sus respectivos cuadros $t$, de tal forma que la columna $t$ del plano $XT$ es la fila $y$ del cuadro $t$ del vídeo. Así mismo el plano $YT$ es otra imagen extraída del vídeo en el mismo pixel $(x,y)$, donde cada columna $t$ de este plano representa un cuadro distinto del vídeo, de tal forma que la columna $t$ del plano $YT$ es la columna $x$ del cuadro $t$ del vídeo. Luego de realizar la extracción de los planos y con la posterior obtención del \textit{rayo de flujo} para el pixel $(x,y)$, fue posible realizar un trazado del movimiento del \textit{rayo} en cada una de sus componentes (En el plano $XT$ se trazo el movimiento de la variable $x$ durante los cuadros del vídeo, por el contrario, en el plano $YT$ se trazó el movimiento de la variable $y$). Ejemplos de estos planos y sus trazos pueden ser vistos en la Tabla~\ref{tabla:comparacion_rayos}, en las columnas (b), (c), (e) y (f).

En general revisando los resultados obtenidos en cada uno de los planos, podemos deducir de forma visual que los \textit{rayos de flujo} modelan de forma aproximada el movimiento de los pixeles. Observando la Tabla~\ref{tabla:comparacion_rayos}, nos dimos cuenta que el modelado de los \textit{rayos} no tiene una mayor variación con respecto a la expresión facial que se esta observando. Al momento de observar las diferencias entre los resultados obtenidos con los vídeos con codificación LBP y sin ésta, pudimos observar que los \textit{rayos} obtenidos en vídeos sin LBP obtenían una mejor aproximación al movimiento real del rayo, este comportamiento puede ser observado al comparar la columna (b) y (e) que representa la extracción en el plano $XT$, o comparando (c) y (f) representantes del plano $YT$ en la Tabla~\ref{tabla:comparacion_rayos}. A su vez se puede observar que las componentes de los \textit{rayos} se comportan mejor al modelar los movimientos verticales (Plano $YT$) que los movimientos horizontales (Plano $XT$), un ejemplo de esto puede ser visto en la expresión (E6), en esta expresión el modelado del movimiento horizontal tiene una aproximación muy baja con respecto a las texturas que se pueden observar en el fondo de la imagen, siendo este un mal modelado de la componente $XT$, por el contrario en la componente del plano $YT$ (columna (c)) se puede observar un modelado casi perfecto con las texturas del fondo de la imagen. Este fenómeno también es visto en otras investigaciones del reconocimiento de expresiones faciales. Ji y Idrissi~\cite{Ji2012} crearon un descriptor espacio temporal que solo utiliza la componente horizontal de los vídeos, ellos se basan en la premisa de que los movimientos significativos del rostro humano tienen una mayor tendencia a ser de forma horizontal que vertical.

\begin{table}[tb]
	\centering
	\begin{tabular}{ >{\centering\arraybackslash}m{2.5cm}  >{\centering\arraybackslash}m{1.2cm}  >{\centering\arraybackslash}m{1.2cm}  >{\centering\arraybackslash}m{1.2cm}  >{\centering\arraybackslash}m{1.2cm}  >{\centering\arraybackslash}m{1.2cm}  >{\centering\arraybackslash}m{1.2cm} }
		\hline
		Codificación & \multicolumn{3}{ c }{XT} & \multicolumn{3}{ c }{YT}\\
		\hline
		& & & & & &\\
		Sin codificación & \includegraphics[width=1.2cm]{Figuras/resultados/comparacion_real/no_lbp/XT/extraido.png} & \includegraphics[width=1.2cm]{Figuras/resultados/comparacion_real/no_lbp/XT/pintado.png} & \includegraphics[width=1.2cm]{Figuras/resultados/comparacion_real/no_lbp/XT/superposicion.png} & \includegraphics[width=1.2cm]{Figuras/resultados/comparacion_real/no_lbp/YT/extraido.png} & \includegraphics[width=1.2cm]{Figuras/resultados/comparacion_real/no_lbp/YT/pintado.png} & \includegraphics[width=1.2cm]{Figuras/resultados/comparacion_real/no_lbp/YT/superposicion.png} \\
		
		Codificación LBP & \includegraphics[width=1.2cm]{Figuras/resultados/comparacion_real/lbp/XT/extraido.png} & \includegraphics[width=1.2cm]{Figuras/resultados/comparacion_real/lbp/XT/pintado.png} & \includegraphics[width=1.2cm]{Figuras/resultados/comparacion_real/lbp/XT/superposicion.png} & \includegraphics[width=1.2cm]{Figuras/resultados/comparacion_real/lbp/YT/extraido.png} & \includegraphics[width=1.2cm]{Figuras/resultados/comparacion_real/lbp/YT/pintado.png} & \includegraphics[width=1.2cm]{Figuras/resultados/comparacion_real/lbp/YT/superposicion.png} \\

		& (i) & (ii) & (iii) & (iv) & (v) & (vi)\\

	\end{tabular}
	\caption{Tabla comparativa de el calculo del error de cada plano. (i) y (iv) representan el \textit{rayo} extraído por el algoritmo; (ii) y (v) el \textit{rayo} promedio dibujado por las personas; (iii) y (vi) la superposición de ambos (de color amarillo el extraído y color azul el promedio). }
	\label{tabla:comparacion_errores}
\end{table}


Para poder ratificar las observaciones que hemos podido deducir al contemplar el movimiento a través de los planos, se diseño un experimento que nos permitió calcular un error aproximado del modelado de los \textit{rayos de flujo}. Este experimento consistió en pedir a distintas personas que dibujaran el \textit{rayo de flujo} sobre cada plano, para esto utilizaron la herramienta de dibujo paint de Windows. En general se pidió a cada una de las personas que dibujaran como creían ellos que se movía el pixel inicial a lo largo de las texturas representadas en el plano. Luego de esta etapa se procedió a extraer cada una de las componentes de los \textit{rayos} dibujados a mano, con esto se logro calcular un promedio de los trazos de las personas. Este \textit{rayo} promedio es comparado con el extraído por el algoritmo, de tal forma que se puede calcular el error para cada componente y un error general. Este proceso puede ser visualizado en la Tabla~\ref{tabla:comparacion_errores}, donde las columnas (iii) y (iv), representan de forma visual como se superpone un \textit{rayo} sobre otro. El error es obtenido calculando la desviación estándar entre ambas componentes. 

\pgfplotstableset{
  franjas/.style={
    columns/errorG/.style={
      column name=Error,
      precision=1
    },
    every even row/.style={
      before row={\rowcolor{#1}}
    },
    every head row/.style={
      before row=\hline\noalign{\smallskip},after row=\hline
    },
    every last row/.style={
      after row=\hline
    }
  },
  franjas/.default={gray!50},
  tablita/.style={
    columns={rs, ws, errorXT, errorYT, errorG},
    columns/rs/.style={
      column name=$RS$,
    },
    columns/ws/.style={
      column name=$WS$,
    },
    columns/errorXT/.style={
      column name=Error $XT$,
      precision=1
    },
    columns/errorYT/.style={
      column name=Error $YT$,
      precision=1
    },
    franjas,
  },
  tablaK/.style={
  	columns={N, K, Accuracy},
  	columns/N/.style={
  		column name=$N$,
  	},
  	columns/K/.style={
  		column name=$K$,
  	},
  	columns/Accuracy/.style={
  		column name=Accuracy,
  		precision=1,
  		postproc cell content/.append style={
  			/pgfplots/table/@cell content/.add={}{\,\%}
  		}
  	},
  	franjas,
  },
  tablaN/.style={
  	columns={RS, WS, N, Accuracy},
  	columns/RS/.style={
  		column name=$RS$,
  	},
  	columns/WS/.style={
  		column name=$WS$,
  	},
  	columns/N/.style={
  		column name=$N$,
  	},
  	columns/Accuracy/.style={
  		column name=Accuracy,
  		precision=1,
  		postproc cell content/.append style={
  			/pgfplots/table/@cell content/.add={}{\,\%}
  		}
  	},
  	franjas,
  },
  tablaSVMresultsRBF/.style={
  	columns={Gamma,C-value,Accuracy},
  	columns/Gamma/.style={
  		column name=$Gamma$,
  		precision=3
  	},
  	columns/C-value/.style={
  		column name=$C-value$,
  		precision=3
  	},
  	columns/Accuracy/.style={
  		column name=$Accuracy$,
  		precision=1,
  		postproc cell content/.append style={
  			/pgfplots/table/@cell content/.add={}{\,\%}
  		}
  	},
  	franjas,
  },
  tablaSVMresultsLineal/.style={
  	columns={C-value,Accuracy},
  	columns/C-value/.style={
  		column name=$C-value$,
  		precision=3
  	},
  	columns/Accuracy/.style={
  		column name=$Accuracy$,
  		precision=1,
  		postproc cell content/.append style={
  			/pgfplots/table/@cell content/.add={}{\,\%}
  		}
  	},
  	franjas,
  },
  tablaSVMresultsPoly/.style={
  	columns={Degree,Gamma,C-value,Accuracy},
  	columns/Degree/.style={
  		column name=$Degree$,
  	},
  	columns/Gamma/.style={
  		column name=$Gamma$,
  		precision=3
  	},
  	columns/C-value/.style={
  		column name=$C-value$,
  		precision=3
  	},
  	columns/Accuracy/.style={
  		column name=$Accuracy$,
  		precision=1,
  		postproc cell content/.append style={
  			/pgfplots/table/@cell content/.add={}{\,\%}
  		}
  	},
  	franjas,
  },
  tablaSVMresults/.style={
  	columns={RS,WS,N,K,Accuracy},
  	columns/RS/.style={
  		column name=$RS$,
  	},
  	columns/WS/.style={
  		column name=$WS$,
  	},
  	columns/N/.style={
  		column name=$N$,
  	},
  	columns/K/.style={
  		column name=$K$,
  	},
  	columns/Accuracy/.style={
  		column name=$Accuracy$,
  		precision=1,
  		postproc cell content/.append style={
  			/pgfplots/table/@cell content/.add={}{\,\%}
  		}
  	},
  	franjas,
  },
}

\begin{table}[tb]
	\centering
	\pgfplotstabletypeset[tablita]{Datos/comparacion_NO_LBP.dat}
	\caption{Tabla comparativa de los errores encontrados en los planos $XT$, $YT$ y error general, al calcular los \textit{rayos} con distintos tamaños de ventanas. Sin codificación LBP.}
	\label{tabla:error_no_lbp}
\end{table}

\begin{table}[tb]
	\centering
	\pgfplotstabletypeset[tablita]{Datos/comparacion_LBP.dat}
	\caption{Tabla comparativa de los errores encontrados en los planos $XT$, $YT$ y error general, al calcular los \textit{rayos} con distintos tamaños de ventanas. Con codificación LBP.}
	\label{tabla:error_lbp}
\end{table}

Decidimos utilizar esta métrica de comparación para visualizar cómo se comportan los \textit{rayos} con respecto al tamaño de las ventanas utilizadas, En las Tablas~\ref{tabla:error_no_lbp}~y~\ref{tabla:error_lbp} se puede ver reflejados los cálculos resultantes del error en ambos planos y el error general calculado como el promedio entre ambos errores, utilizando distintos tamaños de ventanas. Con estos resultados logramos darnos cuenta que la idea planteada sobre las diferencias del modelado vertical y horizontal estaba en lo cierto, en ambas tablas podemos ver que el error obtenido en el plano $XT$ es mucho menor al obtenido en $YT$. Además en general los errores disminuyen a medida que se aumenta el tamaño de las ventanas $RS$ y $WS$, esto es debido a que mientras mas grande sea la región de soporte, menos probabilidad existe de encontrar otra textura igual en la imagen, por lo cual la probabilidad de encontrar en el siguiente cuadro el movimiento de la misma textura es mucho mas alta.

\begin{figure}[t]
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{Figuras/resultados/videos_sinteticos/v1.png}
		\caption{Vídeo sintético simple sin texturas}
		\label{exp:fig:vs1}
	\end{subfigure}
	
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{Figuras/resultados/videos_sinteticos/v2.png}
		\caption{Vídeo sintético simple con texturas}
		\label{exp:fig:vs2}
	\end{subfigure}
	
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{Figuras/resultados/videos_sinteticos/v3.png}
		\caption{Vídeo sintético con texturas.}
		\label{exp:fig:vs3}
	\end{subfigure}
	\caption{Cortes de vídeos sintéticos, en los cuales se describe el movimiento de ciertos pixeles a los largo del tiempo.} 
	\label{exp:fig:vs}
\end{figure}

Luego de obtener resultados satisfactorios con el modelado de los \textit{rayos de flujo}, decidimos probar si la teoría de extracción de estos podía ser aplicada a otros vídeos y otras texturas, por lo que preparamos un conjunto de vídeos sintéticos, los cuales describían movimientos predecibles al ojo humano. En estos vídeos se represento el movimiento de regiones cuadradas con y sin textura, y también se genero un vídeo que permitió examinar que sucede con los rayos si existen dos movimientos distintos.

Luego de realizar la extracción de \textit{rayos de flujo} a los vídeos generados sintéticamente, observamos dos fenómenos. Los rayos pueden modelar el movimiento de los píxeles a los largo del tiempo, esto puede ser visto en la Figura~\ref{exp:fig:vs}, en la cual se ven cortes de distintos cuadros de los vídeos y como se mueven los píxeles escogidos, estos pixeles son representados con cuadros de color a lo largo del vídeo. También descubrimos que el modelado de los \textit{rayos} tiene un comportamiento mas estable cuando existen texturas en las zonas que se están analizando, como se puede ver en las Figuras~\ref{exp:fig:vs2}~y~\ref{exp:fig:vs3}, los píxeles internos del cuadrado siguen durante todo el movimiento en la misma posición del cuadrado, no así en la Figura~\ref{exp:fig:vs1}, en la cual los pixeles que están dentro del cuadrado tienden a juntarse en un punto a medida que el cuadrado se va desplazando. 

En general, por lo que pudimos apreciar en el desarrollo de todos estos experimentos, el comportamiento de los \textit{rayos} depende de dos componentes claves para conseguir un buen modelado, primero una elección inteligente del tamaño de las ventanas $RS$ y $WS$, esto debido a que una ventana más grande disminuyes las probabilidades de error a la hora de elegir el movimiento del pixel, pero a su vez aumenta el costo computacional del algoritmo, debido a que se deben realizar mas cálculos a la hora de la elección del movimiento; segundo, otra componente clave es el nivel de detalle de la textura abarcada por las ventanas, esto debido a que una textura plana puede ser fácilmente confundida con otra textura plana similar, a su vez una textura con detalle permite que el modelado sea más real con respecto a los reales movimientos del rostro humano. 

\subsection{ Normalización de micro-descriptores }

Luego de concretar la extracción de los micro-descriptores, Se procedió a realizar una normalización de los \textit{rayos de flujo}, ésta consistió en poder realizar una reducción o ampliación de los rayos, esto con el fin de que todos los micro-descriptores tengan el mismo largo $N$, ésto nos permitió tener a todos los \textit{rayos} en el mismo espacio vectorial. Para detalles sobre esta etapa revisar la Sección~\ref{algoritmo:normalizacion}.
\begin{table}[b]
	\centering
	\pgfplotstabletypeset[tablaN]{Figuras/resultados/Normalizacion/Accuracy_n.dat}
	\caption{Tabla comparativa de la Asertividad o \textit{Accuracy} (en ingles) obtenido utilizando distintos valores de $N$ y distintos tamaños de las ventanas.}
	\label{tabla:accuracy_N}
\end{table}

Para poder evaluar que valores de la variable $N$ escoger al momento de realizar la normalización, se decidió realizar una comparación entre el valor de la variable $N$ y la asertividad o \textit{Accuracy} reconociendo las expresiones faciales. Estos resultados pueden ser vistos en la Tabla~\ref{tabla:accuracy_N}, para obtener resultados se utilizaron valores de las ventanas $RS$ y $WS$ estudiados en la Sección~\ref{exp:micro-descriptores}. 

Observando los resultados logramos deducir que no existe una gran variación en el \textit{Accuracy} con respecto a los valores de la normalización. En general observamos que el comportamiento de la asertividad no tenia una dependencia muy directa del tamaño de la normalización, esto debido a que este procedimiento lo único que hace es realizar proyección lineal del espacio vectorial sobre un nuevo espacio parametrizado por la variable $N$, visto desde la parte matemática, este procedimiento además de ayudar a poder comparar vectores de distintas dimensiones, permitió realizar una transformación de cada uno de estos \textit{rayos} a un espacio vectorial común para su posterior agrupamiento. 



\subsection{ Creación de macro-descriptores }

La creación de los macro-descriptores es la etapa final de la extracción de características de los vídeos, se utilizó la técnica \textit{Bag of visual words} analizada en la Sección~\ref{sec:bag_of_words}, para obtener un diccionario que permita representar el espacio vectorial a analizar, en el caso de nuestra implementación es necesario obtener un conjunto de \textit{rayos de flujo} representativos del espacio, este conjunto es llamado vocabulario, y en el se encuentran todas las posibles palabras que pueden existir en el universo formado por los datos de entradas. Para poder obtener este vocabulario sobre nuestros micro-descriptores, se utilizó la técnica de agrupamiento no supervisada $k$-means analizada en la Seccion~\ref{sec:k-means}, la cual recibe como entrada un grupo de vectores (\textit{rayos de flujo}) y la cantidad $K$ de grupos a formar, con esto calcula cada uno de los vectores o \textit{rayos} representantes, los cuales son el vocabulario utilizado por el \textit{Bag of visual words}. Luego de obtener este vocabulario, se procede a construir los macro-descriptores de cada vídeo, calculando un histograma de tamaño $K$, el cual es la representación de la frecuencia de asociación de cada \textit{rayo} con su respectivo \textit{cluster} $k$. Para mas detalles de esta etapa revisar la Sección~\ref{sec:macro-descriptores}. Con esto logramos deducir que la variable $K$, es de suma importancia para el algoritmo, esto debido a que la construcción de los descriptores finales de cada vídeo dependen del valor asignado a esta variable.

\newlength{\st}
\setlength{\st}{1.5cm}
\setlength{\sz}{1.6cm}
\begin{table}[tb]
	\centering
	\begin{tabular}{ >{\centering\arraybackslash}m{.5cm}  >{\centering\arraybackslash}m{\st}  >{\centering\arraybackslash}m{\st}  >{\centering\arraybackslash}m{\st}  >{\centering\arraybackslash}m{\st}  >{\centering\arraybackslash}m{\st}  >{\centering\arraybackslash}m{\st}  }
		\hline\noalign{\smallskip}
		$K$ & Ira & Asco & Miedo & Alegría & Tristeza & Sorpresa\\
		\hline\noalign{\smallskip}
		10 & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=10/E1_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=10/E2_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=10/E3_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=10/E4_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=10/E5_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=10/E6_ec.png} \\
		
		50 & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=50/E1_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=50/E2_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=50/E3_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=50/E4_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=50/E5_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=50/E6_ec.png} \\
		
		100 & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=100/E1_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=100/E2_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=100/E3_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=100/E4_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=100/E5_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=100/E6_ec.png} \\
		
		500 & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=500/E1_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=500/E2_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=500/E3_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=500/E4_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=500/E5_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=500/E6_ec.png} \\
		
		1000 & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=1000/E1_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=1000/E2_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=1000/E3_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=1000/E4_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=1000/E5_ec.png} & \includegraphics[height=\sz]{Figuras/resultados/clustering/k=1000/E6_ec.png} \\
		
		& (E1) & (E2) & (E3) & (E4) & (E5) & (E6) \\
	\end{tabular}
	\caption{Tabla comparativa de la cantidad de \textit{cluster} utilizados y su representación en las distintas expresiones faciales. Las imágenes fueron construidas en escala de grises, los colores mas cercanos al blanco representan mayor cantidad de \textit{rayos} agrupados en ese conjunto. }
	\label{tabla:comparacion_clusters} 
\end{table}	

\begin{table}[tb]
	\centering
	\pgfplotstabletypeset[tablaK]{Figuras/resultados/clustering/Accuracy_k.dat}
	\caption{Tabla comparativa de la Asertividad o \textit{Accuracy} (en ingles) obtenido utilizando distintos valores de $K$.}
	\label{tabla:accuracy_K}
\end{table}

Para poder evaluar si esta técnica realizó un modelado exitoso preparamos dos experimentos, el primero consistió en poder realizar una evaluación visual del movimiento de los grupos o \textit{clusters} de \textit{rayos}, siendo estos representados sobre el rostro que fue analizado. En general sabemos por la extracción de \textit{rayos de flujo}, que cada uno de estos tiene asociada una posición inicial dentro del rostro analizado, con lo cual se procedió a crear imágenes en escala de grises, las cuales representan los \textit{clusters} dentro del vídeo. Algunos ejemplos pueden ser vistos en la Tabla~\ref{tabla:comparacion_clusters}. 

Las imágenes fueron construidas calculando y ordenando los $P$ grupos con mayor frecuencia ( 0 <\ $P$ <\ 255 y $P \leq K$ ), cada grupo $p$ fue asignado a un valor en la escala de grises, a los grupos que obtuvieron una mayor frecuencia de aparición les fueron asignado valores mas cercanos a $255$ (Color blanco), del mismo modo los \textit{clusters} con menor frecuencia obtienen valores mas cercanos a $0$ (Color negro), a su vez todos los demás grupos que quedaron fuera de la selección inicial reciben el valor $0$. Esto debido a que los \textit{cluster} con menor frecuencia pueden ser catalogados como \textit{rayos} ruidosos, y que no aportan información al sistema.

Observando las imágenes obtenidas y representadas en la Tabla~\ref{tabla:comparacion_clusters}, logramos percatarnos que existe una gran segmentación de las regiones más importantes del rostro (Ojos, cejas, nariz y boca). A medida que la variable $K$ aumenta, logramos visualizar de mejor manera las regiones de interés. El ejemplo mas característico puede ser visualizado en la expresión de sorpresa (E6), cuando $K$ tomó el valor 10, en (E6) observamos una tendencia de los \textit{clusters} a agruparse en las regiones importantes, conforme fue aumentando el valor de $K$, la imagen del rostro se fue volviendo mas nítida llegando a detectar visualmente de forma muy directa las partes importantes del rostro (véase la secuencia de imágenes de (E6) con distintos valores $K$), así mismo esto paso con el general de los vídeos utilizados en los experimentos. También logramos deducir que visualmente no existe una dependencia entre la expresión facial que se quiso representar y las regiones del rostro más nítidas.

Cabe destacar que por las observaciones realizadas, es de vital importancia una elección inteligente de la cantidad de grupos a crear, esto debido a que un valor muy pequeño puede hacer que los conjuntos encontrados estén muy poco segmentados (Ver k = 10 en la Tabla~\ref{tabla:comparacion_clusters}), por el contrario un valor muy alto puede provocar una sobre segmentación de los \textit{cluster}, de tal forma que los \textit{rayos} estén tan segmentados que los clasificadores no puedan encontrar patrones en los macro-descriptores de la misma expresión.

El segundo experimento consistió en buscar de forma mas empírica un valor que permita la creación de un buen modelo de clasificación a la hora de etiquetar nuevas expresiones entrantes. Para esto se realizó una comparación de distintos valores $K$ y su asertividad a la hora de clasificar. Estos resultados son expuestos en la Tabla~\ref{tabla:accuracy_K}, y fueron obtenidos al utilizar los mejores valores resultantes en las variables analizadas en las secciones anteriores de este capítulo, los datos obtenidos son bastante pobres a pesar de los cambios de la variable $K$, esto puede ser debido a un mal agrupamiento por parte del algoritmo. Es de vital importancia mencionar que los algoritmos de \textit{clustering} son muy sensibles a las semillas ingresadas como puntos iniciales, para estos experimentos utilizamos un algoritmo de elección inteligente de las semillas, este método esta basado en las investigaciones de Arthur y Vassilvitskii~\cite{Arthur2007}, el cual esta implementado en la librería OpenCV.

En general, luego de observar los resultados de los experimentos expuestos, se pudo visualizar una necesidad de seguir profundizando en el tema de como construir los macro-descriptores, proponemos realizar investigaciones sobre los distintos algoritmos que se pueden utilizar a la hora de agrupar, cuales medidas de distancia permiten un mejor agrupamiento de los \textit{rayos}, buscar como inicializar de mejor manera las semillas. Por otra parte revisando los resultados obtenidos al realizar el mapeo de los \textit{clusters} en el rostro surgieron dudas sobre que es lo que pasaría si la extracción de rayos se realizara solo sobre las regiones importantes del rostro, esto ¿daría pie a un agrupamiento mas localizado sobre las regiones que realmente nos interesa saber su movimiento?. 


\subsection{ Entrenamiento y clasificación }


\begin{table}[tb]
	\centering
	\pgfplotstabletypeset[tablaSVMresultsRBF]{Datos/rbf_10.csv}
	\caption{Mejores resultados obtenidos por el Kernel RBF.}
	\label{tabla:svm_rbf}
\end{table}
\begin{table}[tb]
	\centering
	\pgfplotstabletypeset[tablaSVMresultsLineal]{Datos/lineal_10.csv}
	\caption{Mejores resultados obtenidos por el Kernel Lineal.}
	\label{tabla:svm_lineal}
\end{table}
\begin{table}[tb]
	\centering
	\pgfplotstabletypeset[tablaSVMresultsPoly]{Datos/poly_10.csv}
	\caption{Mejores resultados obtenidos por el Kernel Polinomial.}
	\label{tabla:svm_poly}
\end{table}

El entrenamiento y clasificación es de vital importancia, debido a que en esta etapa es cuando se genera el modelo que permite clasificar las nuevas expresiones entrantes. Este modulo puede ser dividido en dos partes, primero el entrenamiento, el cual es el encargado de encontrar un modelo que permita clasificar de la mejor forma las nuevas entradas; segundo la clasificación, etapa encargada de etiqueta con alguna expresión las nuevas entradas, esta etiqueta es asignada dependiendo del modelo entrenado en la fase anterior. También en esta etapa se realiza el calculo de la asertividad o \textit{Accuracy} (Por sus siglas en ingles) del algoritmo. Para obtener un calculo mas real del modelo generado se utilizó la técnica de $k$-fold cross-validation, la cual en simples palabras consiste en separar los datos de forma aleatoria en dos grupos (Entrenamiento y pruebas), realizando esta separación $k$ veces y para cada una de estas separaciones obteniendo un \textit{Accuracy} el cual es el promediado de todas separaciones, este promedio es el promedio que se utiliza como medida de comparación. Para todos los experimentos expuestos en esta Sección se utilizo un 10-fold cross validación y SVM. Para mayores detalles de esta etapa, revisar la Sección~\ref{sec:clasificacion}.

Como se explico en la Sección~\ref{sec:svm}, SVM trata de encontrar una recta que permita separar a los dos grupos datos dentro de un mismo espacio vectorial, en el general de los casos reales no existe una recta que permita separar de forma efectiva ambas clases, por esto SVM hace uso de las técnicas de Kernel, en simples palabras estas técnicas permiten realizar transformación lineales de los espacios vectoriales, con lo cual se puede realizar la búsqueda de un hiper-plano que logre segmentar ambas clases y minimice el error. Debido a esto realizamos un experimento que permita encontrar de forma empírica cual de los Kernel y valores de sus respectivas variables maximicen el \textit{Accuracy}, Los Kernels utilizados están implementados en la librería en Opencv la cual se basa LibSVM creada por Chang~y~Lin~\cite{Chang2011} para obtener resultados de SVM.


Como se puede observar en las Tablas~\ref{tabla:svm_rbf},~\ref{tabla:svm_lineal}~y~\ref{tabla:svm_poly}, se obtuvieron resultados muy bajos, en general, los mejores \textit{Accuracy} fueron obtenidos utilizando el Kernel RBF, el cual permite realizar una transformación lineal, donde una recta en el espacio del Kernel puede ser representada por una circunferencia en el plano real de los datos. Revisando los valores de las variables ($\gamma$, \textit{Degree} y $C$-value), observamos que no existe una segmentación clara sobre que valores deben adquirir estas variables para obtener mejores resultado a la hora de entrenar el modelo. En la Sección~\ref{sec:svm}, nombramos distintos kernels que podrían ser utilizados para el problema de clasificación, entre ellos \textit{Histogram Intersection}, el cual tiene una gran efectividad a la hora de generar un modelo de vectores representantes de histogramas, este kernel no esta incluido en la implementación de SVM en la librería OpenCV. Por lo cual es importante buscar alguna implementan de SVM que tenga incluido este kernel.

\begin{table}[tb]
	\centering
	\pgfplotstabletypeset[tablaSVMresults]{Datos/resultados.dat}
	\caption{Resultados obtenidos utilizando las mejores variables obtenidas en el proceso de experimentos.}
	\label{tabla:exp:accuracy}
\end{table}


Por ultimo como experimento final decidimos realizar una comparación del \textit{Accuracy} obtenido con mejores valores de las variables del algoritmo ($RS$, $WS$, $N$, $K$). Esta comparación puede ser visualizada en la Tabla~\ref{tabla:exp:accuracy}.

En general y al igual que en la mayoría de los experimentos de obtención de \textit{Accuracy}, se obtuvieron resultados muy pobres, esto puede ser debido a una mala aproximacion de los valores de las variables de los kernels, o que los kernels que se utilizan no permiten realizar un mapeo exitoso del espacio vectorial observado. Otro posible problema puede ser el tamaño de los conjuntos de entrenamiento y prueba utilizados por el $k$-fold cross validation, debido a que para todos los experimentos se utilizo un 50\% de los videos para el entrenamiento, por lo cual puede existir una necesidad del clasificador por tener mas datos de entrenamiento, para así poder obtener un mejor modelo de clasificación.
